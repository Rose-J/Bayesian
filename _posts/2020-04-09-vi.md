---
layout: post
title: Variational Inference
categories: background
use_math: True
---

data : $D$  
model parameters : $\theta$  

prior  : $p(\theta)$  
likelihood : $p(D|\theta)$  
posterior :  $p(\theta|D)$  

### Bayes' rule

$$
P(\theta|D) = \frac{p(D|\theta)p(\theta)}{p(D)} = \frac{p(D|\theta)p(\theta)}{\int p(D|\theta)p(\theta)}
$$  

The denominator term is intractable.

### Baisics

$$
p(D) = \frac{p(D|\theta)p(\theta)}{p(\theta|D)}
$$

$$
p(D) = \int q(z)p(D)dz
$$

### Variational Inference

$$
\begin{aligned}
logP(D) &= \int q(\theta)log(p(D))d\theta \\ 
			  &= \int q(\theta)log(\frac{p(D|\theta)p(\theta)}{p(\theta|D)})d\theta \\
			  &= \int q(\theta)log(p(D|\theta)) + q(\theta)log(p(\theta)) -q(\theta)log(p(\theta|D)d\theta \\
			  &= \int q(\theta)log(p(D|\theta)) + q(\theta)log(p(\theta)) -q(\theta)log(p(\theta|D) + q(\theta)log(q(\theta)) - q(\theta)log(q(\theta))d\theta \\
			  &= \int q(\theta)log(p(D|\theta)) -q(\theta)log(\frac{q(\theta)}{p(\theta)}) + q(\theta)log(\frac{q(\theta)}{p(\theta|D)})d\theta \\
			  &=E_{q(\theta)}[log(p(D|\theta)]-KL(q(\theta)||p(\theta) + KL(q(\theta)||p(\theta||D) \\
			  &\geq E_{q(\theta)}[log(p(D|\theta)]-KL(q(\theta)||p(\theta) \rightarrow \mathrm{ELBO(Evidence Lower Bound)}
\end{aligned}
$$
